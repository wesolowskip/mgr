+ CONTAINER=/home2/faculty/pwesolowski/containers/cuml-prod.sif
+ CONTAINER_RC_FILE=/home2/faculty/pwesolowski/containers/singularity_rc
+ SCRIPT=/home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ for FILES in "California.json" ""
+ for PROTOCOL in tcp ucx
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2251276)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 08:15:13,599 - distributed.diskutils - INFO - Found stale lock file and directory '/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-mqm8j3cm', purging
2023-07-04 08:15:13,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:15:13,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 37604.80186 ms
Code block 'ddf-preprocessing' took: 15359.57798 ms
Code block 'ddf-preprocessing' took: 15288.61916 ms
Code block 'ddf-preprocessing' took: 15401.50010 ms
Code block 'ddf-preprocessing' took: 15347.36505 ms
Code block 'ddf-scaler-fit' took: 17769.63732 ms
Code block 'ddf-scaler-fit' took: 13812.44539 ms
Code block 'ddf-scaler-fit' took: 14571.99302 ms
Code block 'ddf-scaler-fit' took: 14945.24595 ms
Code block 'ddf-scaler-fit' took: 15477.99689 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45807.84678 ms
Code block 'ddf-kmeans-score' took: 15116.68693 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41882.84149 ms
Code block 'ddf-kmeans-score' took: 15248.72713 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41927.25550 ms
Code block 'ddf-kmeans-score' took: 15242.06473 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42008.01973 ms
Code block 'ddf-kmeans-score' took: 15398.83101 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42048.60709 ms
Code block 'ddf-kmeans-score' took: 15340.78044 ms
scores=[array(-4836262.0625)]
client.get_worker_logs()={'tcp://127.0.0.1:33903': (('INFO', "2023-07-04 08:22:52,240 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:22:25,058 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:21:53,752 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:21:26,463 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:20:55,460 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:20:28,224 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:19:57,078 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:19:29,847 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:18:59,036 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:18:28,201 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:15:15,145 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:15:15,145 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45149'), ('INFO', '2023-07-04 08:15:15,115 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:15:15,115 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a47de60d-b3bd-4fca-8791-a2ad50c09d3b'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO - Starting Worker plugin PreImport-f6be3cae-ddef-49e4-8d84-1f18244c9fb4'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a95ad48-25e4-4a03-b03b-29054072b169'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-9h251asl'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45149'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37269'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33903'), ('INFO', '2023-07-04 08:15:14,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33903'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2260341)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 08:23:35,139 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:23:35,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 39039.46486 ms
Code block 'ddf-preprocessing' took: 15227.58914 ms
Code block 'ddf-preprocessing' took: 15317.06546 ms
Code block 'ddf-preprocessing' took: 15215.57978 ms
Code block 'ddf-preprocessing' took: 15240.71014 ms
Code block 'ddf-scaler-fit' took: 20477.11769 ms
Code block 'ddf-scaler-fit' took: 16762.98159 ms
Code block 'ddf-scaler-fit' took: 17109.90292 ms
Code block 'ddf-scaler-fit' took: 17618.55020 ms
Code block 'ddf-scaler-fit' took: 18085.49611 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 48230.67099 ms
Code block 'ddf-kmeans-score' took: 17659.31352 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44633.37225 ms
Code block 'ddf-kmeans-score' took: 18024.21893 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44762.21910 ms
Code block 'ddf-kmeans-score' took: 17807.66860 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44739.33561 ms
Code block 'ddf-kmeans-score' took: 18079.09877 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44738.17294 ms
Code block 'ddf-kmeans-score' took: 17990.90008 ms
scores=[array(-4838706.2734375)]
client.get_worker_logs()={'tcp://127.0.0.1:41831': (('INFO', "2023-07-04 08:31:52,134 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:31:24,848 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:30:48,285 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:30:21,051 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:29:44,697 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:29:17,482 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:28:40,871 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:28:13,618 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:27:37,541 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:27:06,814 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:23:36,723 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:23:36,723 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40207'), ('INFO', '2023-07-04 08:23:36,691 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO - Starting Worker plugin PreImport-3b591f54-3390-492e-a997-97d8999356bb'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-609ba06a-b6b5-453f-8cb9-cf4dc251f16d'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf845909-8b7d-4e54-9e47-9f6fa6467bf7'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-yn6mms_w'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40207'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33415'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41831'), ('INFO', '2023-07-04 08:23:35,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41831'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2269939)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 08:32:39,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:32:39,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 37424.44948 ms
Code block 'ddf-preprocessing' took: 15369.65437 ms
Code block 'ddf-preprocessing' took: 15218.23401 ms
Code block 'ddf-preprocessing' took: 15066.08618 ms
Code block 'ddf-preprocessing' took: 15157.46167 ms
Code block 'ddf-scaler-fit' took: 15893.31271 ms
Code block 'ddf-scaler-fit' took: 11609.03103 ms
Code block 'ddf-scaler-fit' took: 11713.50500 ms
Code block 'ddf-scaler-fit' took: 12126.48069 ms
Code block 'ddf-scaler-fit' took: 12197.90675 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43068.39378 ms
Code block 'ddf-kmeans-score' took: 12472.45384 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39345.19299 ms
Code block 'ddf-kmeans-score' took: 12444.82571 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39461.27217 ms
Code block 'ddf-kmeans-score' took: 12423.60412 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39267.44367 ms
Code block 'ddf-kmeans-score' took: 12371.93453 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39324.35672 ms
Code block 'ddf-kmeans-score' took: 12379.06623 ms
scores=[array(-4837981.84375)]
client.get_worker_logs()={'tcp://127.0.0.1:38331': (('INFO', "2023-07-04 08:39:39,630 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:39:12,455 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:38:46,940 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:38:19,695 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:37:54,264 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:37:27,013 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:37:01,351 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:36:34,122 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:36:08,531 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:35:37,697 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:32:41,248 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:32:41,247 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34431'), ('INFO', '2023-07-04 08:32:41,216 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO - Starting Worker plugin PreImport-755d1c09-45cd-41b1-a22e-0846178ccf8d'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23aac50b-a3c7-4547-be00-5d8bab480b6d'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58eda5a0-89bc-4a7c-8f96-c35c6071a42a'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-3v01r43m'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34431'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33143'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38331'), ('INFO', '2023-07-04 08:32:40,290 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38331'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2278072)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 08:40:17,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:40:17,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 36908.81795 ms
Code block 'ddf-preprocessing' took: 15124.46542 ms
Code block 'ddf-preprocessing' took: 15230.70082 ms
Code block 'ddf-preprocessing' took: 15005.93453 ms
Code block 'ddf-preprocessing' took: 15210.65458 ms
Code block 'ddf-scaler-fit' took: 18592.11373 ms
Code block 'ddf-scaler-fit' took: 14302.05667 ms
Code block 'ddf-scaler-fit' took: 14387.69041 ms
Code block 'ddf-scaler-fit' took: 15518.53175 ms
Code block 'ddf-scaler-fit' took: 14758.48591 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45533.40973 ms
Code block 'ddf-kmeans-score' took: 14940.59102 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41981.77664 ms
Code block 'ddf-kmeans-score' took: 15030.93436 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41850.59481 ms
Code block 'ddf-kmeans-score' took: 15091.88306 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41827.32221 ms
Code block 'ddf-kmeans-score' took: 14939.11289 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41864.80988 ms
Code block 'ddf-kmeans-score' took: 15086.15692 ms
scores=[array(-4836132.5625)]
client.get_worker_logs()={'tcp://127.0.0.1:44875': (('INFO', "2023-07-04 08:47:53,718 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:47:26,578 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:46:55,926 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:46:28,787 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:45:58,022 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:45:30,776 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:45:00,026 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:44:32,790 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:44:02,105 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:43:31,402 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:40:19,108 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:40:19,108 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41111'), ('INFO', '2023-07-04 08:40:19,079 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO - Starting Worker plugin PreImport-7ae1d998-66f0-44b4-8e94-3dd8e30a77f7'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d5d8061-206d-453b-9787-03b2a319168f'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7fd54984-c7c1-4d7d-b003-e3f4df604665'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-yyl55vok'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41111'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33709'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44875'), ('INFO', '2023-07-04 08:40:18,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44875'))}
+ sleep 1
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2286725)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 08:48:38,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:48:38,344 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 41713.74891 ms
Code block 'ddf-preprocessing' took: 15405.14259 ms
Code block 'ddf-preprocessing' took: 15511.73567 ms
Code block 'ddf-preprocessing' took: 15582.23202 ms
Code block 'ddf-preprocessing' took: 15472.19901 ms
Code block 'ddf-scaler-fit' took: 16467.85019 ms
Code block 'ddf-scaler-fit' took: 12300.79932 ms
Code block 'ddf-scaler-fit' took: 12711.80163 ms
Code block 'ddf-scaler-fit' took: 13104.23610 ms
Code block 'ddf-scaler-fit' took: 13254.89044 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43146.14291 ms
Code block 'ddf-kmeans-score' took: 12167.40417 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39145.38275 ms
Code block 'ddf-kmeans-score' took: 12419.21083 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39115.36974 ms
Code block 'ddf-kmeans-score' took: 12180.49014 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39442.77638 ms
Code block 'ddf-kmeans-score' took: 12315.81159 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39173.59752 ms
Code block 'ddf-kmeans-score' took: 12244.64364 ms
scores=[array(-4836798.109375)]
client.get_worker_logs()={'tcp://127.0.0.1:38083': (('INFO', "2023-07-04 08:55:47,430 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:55:20,222 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:54:54,951 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:54:27,688 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:54:02,338 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:53:35,071 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:53:09,804 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:52:42,547 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 08:52:17,492 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:51:46,608 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:48:40,327 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:48:40,327 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35015'), ('INFO', '2023-07-04 08:48:40,294 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:48:39,317 - distributed.worker - INFO - Starting Worker plugin PreImport-42145a76-9f63-4c17-8d48-84108a0cc12d'), ('INFO', '2023-07-04 08:48:39,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3784bdcc-aa4b-4364-98e7-fd9e69680d8f'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c24d5a3-01b3-4083-ba4a-5a83241bb0b9'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-sy3je1nx'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35015'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34117'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38083'), ('INFO', '2023-07-04 08:48:38,973 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38083'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2295493)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 08:56:26,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 08:56:26,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 31974.12964 ms
Code block 'ddf-preprocessing' took: 15370.39642 ms
Code block 'ddf-preprocessing' took: 15294.57334 ms
Code block 'ddf-preprocessing' took: 15342.40766 ms
Code block 'ddf-preprocessing' took: 15273.64665 ms
Code block 'ddf-scaler-fit' took: 18330.73476 ms
Code block 'ddf-scaler-fit' took: 14942.38741 ms
Code block 'ddf-scaler-fit' took: 15453.45607 ms
Code block 'ddf-scaler-fit' took: 15606.42996 ms
Code block 'ddf-scaler-fit' took: 15957.63575 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45707.62208 ms
Code block 'ddf-kmeans-score' took: 14925.82171 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41741.17956 ms
Code block 'ddf-kmeans-score' took: 14917.00389 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41878.11688 ms
Code block 'ddf-kmeans-score' took: 14997.90479 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42046.97176 ms
Code block 'ddf-kmeans-score' took: 15024.02813 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41852.16392 ms
Code block 'ddf-kmeans-score' took: 15019.93429 ms
scores=[array(-4838301.2578125)]
client.get_worker_logs()={'tcp://127.0.0.1:44565': (('INFO', "2023-07-04 09:04:07,480 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:03:40,322 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:03:09,611 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:02:42,418 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:02:11,566 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:01:44,356 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:01:13,618 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:00:46,445 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:00:15,952 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 08:59:45,050 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 08:56:34,202 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:56:34,202 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42295'), ('INFO', '2023-07-04 08:56:34,166 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:56:28,035 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a38aabf7-ec48-4cf0-9b87-2216d93f922c'), ('INFO', '2023-07-04 08:56:28,035 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57b52f74-e427-4266-a3ff-40d7720dc7b1'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO - Starting Worker plugin PreImport-e9ae827a-94c7-43a5-90cb-840969169594'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-kn01kj4u'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42295'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37539'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44565'), ('INFO', '2023-07-04 08:56:26,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44565'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2304482)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 09:04:50,064 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:04:50,065 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 37610.93575 ms
Code block 'ddf-preprocessing' took: 15089.89156 ms
Code block 'ddf-preprocessing' took: 15053.64174 ms
Code block 'ddf-preprocessing' took: 15126.72351 ms
Code block 'ddf-preprocessing' took: 15045.48696 ms
Code block 'ddf-scaler-fit' took: 15386.98207 ms
Code block 'ddf-scaler-fit' took: 10796.32369 ms
Code block 'ddf-scaler-fit' took: 10922.04526 ms
Code block 'ddf-scaler-fit' took: 10903.66885 ms
Code block 'ddf-scaler-fit' took: 11026.57596 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41599.17029 ms
Code block 'ddf-kmeans-score' took: 10759.52490 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37782.55401 ms
Code block 'ddf-kmeans-score' took: 10755.66287 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37778.83370 ms
Code block 'ddf-kmeans-score' took: 10794.03673 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37685.14606 ms
Code block 'ddf-kmeans-score' took: 10901.67343 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37889.17161 ms
Code block 'ddf-kmeans-score' took: 10746.98891 ms
scores=[array(-4839286.09375)]
client.get_worker_logs()={'tcp://127.0.0.1:45959': (('INFO', "2023-07-04 09:11:31,295 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:11:04,148 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:10:41,544 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:10:14,492 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:09:52,107 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:09:25,049 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:09:02,611 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:08:35,449 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:08:12,954 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:07:42,226 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:04:51,983 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:04:51,983 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43359'), ('INFO', '2023-07-04 09:04:51,950 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:04:51,002 - distributed.worker - INFO - Starting Worker plugin PreImport-4831bcc6-e7f0-4400-a6c8-46a1330da9f4'), ('INFO', '2023-07-04 09:04:51,002 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c214a20-8277-4fd6-b94a-733144cfb390'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09cd79ca-3713-4971-986e-86b140987225'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-xb4p7xdd'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43359'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33705'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45959'), ('INFO', '2023-07-04 09:04:50,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45959'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=California.json
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2312613)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol tcp --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 09:12:08,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:12:08,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 32174.83477 ms
Code block 'ddf-preprocessing' took: 15118.80210 ms
Code block 'ddf-preprocessing' took: 15219.04851 ms
Code block 'ddf-preprocessing' took: 15194.30187 ms
Code block 'ddf-preprocessing' took: 15292.86837 ms
Code block 'ddf-scaler-fit' took: 17288.78708 ms
Code block 'ddf-scaler-fit' took: 13680.92290 ms
Code block 'ddf-scaler-fit' took: 13745.01180 ms
Code block 'ddf-scaler-fit' took: 13911.15356 ms
Code block 'ddf-scaler-fit' took: 14120.99125 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44712.34530 ms
Code block 'ddf-kmeans-score' took: 13598.05545 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40524.70930 ms
Code block 'ddf-kmeans-score' took: 13675.53583 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40633.22732 ms
Code block 'ddf-kmeans-score' took: 13669.20082 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40698.06544 ms
Code block 'ddf-kmeans-score' took: 13665.97108 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40567.84856 ms
Code block 'ddf-kmeans-score' took: 13645.13974 ms
scores=[array(-4838139.65625)]
client.get_worker_logs()={'tcp://127.0.0.1:42985': (('INFO', "2023-07-04 09:19:30,069 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:19:02,924 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:18:34,752 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:18:07,618 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:17:39,428 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:17:12,217 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:16:44,158 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:16:16,961 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:15:48,919 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:15:17,946 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:12:16,077 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:12:16,077 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42859'), ('INFO', '2023-07-04 09:12:16,047 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:12:09,724 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03073fb1-6bcb-4fd7-806a-3004428cb77b'), ('INFO', '2023-07-04 09:12:09,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b867c60a-1764-41f9-a87e-09b95e06c2bd'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO - Starting Worker plugin PreImport-2948acc2-921c-4194-ae40-0303f187f2b7'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ifqfplrp'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42859'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33051'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:12:08,802 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42985'), ('INFO', '2023-07-04 09:12:08,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42985'))}
+ sleep 1
+ for PROTOCOL in tcp ucx
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2321206)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 09:20:11,722 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2322290. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688455212.151400] [dgx-4:2322290:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 09:20:13,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:20:13,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-356' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 37324.08752 ms
Code block 'ddf-preprocessing' took: 15314.43215 ms
Code block 'ddf-preprocessing' took: 15310.98646 ms
Code block 'ddf-preprocessing' took: 15432.83296 ms
Code block 'ddf-preprocessing' took: 15264.15108 ms
Code block 'ddf-scaler-fit' took: 17949.18124 ms
Code block 'ddf-scaler-fit' took: 13899.37000 ms
Code block 'ddf-scaler-fit' took: 14744.81271 ms
Code block 'ddf-scaler-fit' took: 15106.77908 ms
Code block 'ddf-scaler-fit' took: 15853.42174 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45796.67783 ms
Code block 'ddf-kmeans-score' took: 15248.07704 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41966.94836 ms
Code block 'ddf-kmeans-score' took: 15331.29935 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42254.91356 ms
Code block 'ddf-kmeans-score' took: 15725.87307 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42352.96129 ms
Code block 'ddf-kmeans-score' took: 15763.29230 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42253.03634 ms
Code block 'ddf-kmeans-score' took: 15769.69488 ms
scores=[array(-4836640.71875)]
client.get_worker_logs()={'ucx://127.0.0.1:54501': (('INFO', "2023-07-04 09:27:55,379 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:27:28,152 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:26:56,160 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:26:28,918 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:25:56,978 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:25:29,693 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:24:58,067 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:24:30,777 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:23:59,774 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:23:28,901 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:20:15,182 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:20:15,182 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:38649'), ('INFO', '2023-07-04 09:20:15,091 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:20:14,176 - distributed.worker - INFO - Starting Worker plugin PreImport-eb9ff7d7-eb5e-4072-9f80-27a18d66901f'), ('INFO', '2023-07-04 09:20:14,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16883ced-0038-4381-8240-5078a3c55fd9'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a9320bd-d420-4db9-86fe-50251019bb57'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ob85zjw4'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:38649'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36309'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:54501'), ('INFO', '2023-07-04 09:20:14,175 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:54501'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2330101)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 09:28:37,230 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2331302. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688455717.692373] [dgx-4:2331302:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 09:28:39,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:28:39,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-353' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 37868.51919 ms
Code block 'ddf-preprocessing' took: 15286.49758 ms
Code block 'ddf-preprocessing' took: 15388.23408 ms
Code block 'ddf-preprocessing' took: 15266.02309 ms
Code block 'ddf-preprocessing' took: 15377.15243 ms
Code block 'ddf-scaler-fit' took: 20645.03010 ms
Code block 'ddf-scaler-fit' took: 17012.86152 ms
Code block 'ddf-scaler-fit' took: 17435.20118 ms
Code block 'ddf-scaler-fit' took: 18027.33792 ms
Code block 'ddf-scaler-fit' took: 18479.02189 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 48814.19291 ms
Code block 'ddf-kmeans-score' took: 18129.48573 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44904.47156 ms
Code block 'ddf-kmeans-score' took: 18142.03432 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44908.48248 ms
Code block 'ddf-kmeans-score' took: 18217.69391 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44870.98895 ms
Code block 'ddf-kmeans-score' took: 18155.69249 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45016.83516 ms
Code block 'ddf-kmeans-score' took: 18137.88053 ms
scores=[array(-4838543.484375)]
client.get_worker_logs()={'ucx://127.0.0.1:43721': (('INFO', "2023-07-04 09:36:59,756 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:36:32,477 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:35:55,499 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:35:28,238 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:34:51,315 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:34:24,016 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:33:47,161 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:33:19,953 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:32:42,889 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:32:11,932 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:28:40,704 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:28:40,704 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:35591'), ('INFO', '2023-07-04 09:28:40,627 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:28:40,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f521e11-418f-493b-bceb-d1dddeecd80f'), ('INFO', '2023-07-04 09:28:40,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c147e8b4-dab1-40d6-a4bd-337a39084c13'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO - Starting Worker plugin PreImport-39211d43-18a4-408e-9048-f22d7476bdfa'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-nfbi_9uy'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:35591'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39477'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:43721'), ('INFO', '2023-07-04 09:28:39,708 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:43721'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2340054)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 09:37:45,788 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2341211. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688456266.319726] [dgx-4:2341211:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 09:37:47,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:37:47,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-356' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 40021.27571 ms
Code block 'ddf-preprocessing' took: 15187.11693 ms
Code block 'ddf-preprocessing' took: 15069.15151 ms
Code block 'ddf-preprocessing' took: 15190.80220 ms
Code block 'ddf-preprocessing' took: 15052.29757 ms
Code block 'ddf-scaler-fit' took: 16009.92743 ms
Code block 'ddf-scaler-fit' took: 11599.35178 ms
Code block 'ddf-scaler-fit' took: 11880.77083 ms
Code block 'ddf-scaler-fit' took: 12213.10784 ms
Code block 'ddf-scaler-fit' took: 12365.43141 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43117.23342 ms
Code block 'ddf-kmeans-score' took: 12492.75939 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39202.98883 ms
Code block 'ddf-kmeans-score' took: 12520.59555 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39386.04736 ms
Code block 'ddf-kmeans-score' took: 12401.41841 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39280.96909 ms
Code block 'ddf-kmeans-score' took: 12482.86489 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39299.35945 ms
Code block 'ddf-kmeans-score' took: 12404.83335 ms
scores=[array(-4837519.875)]
client.get_worker_logs()={'ucx://127.0.0.1:54895': (('INFO', "2023-07-04 09:44:50,937 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:44:23,725 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:43:58,129 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:43:30,950 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:43:05,423 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:42:38,254 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:42:12,467 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:41:45,291 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:41:19,590 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:40:48,818 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:37:49,368 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:37:49,368 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:37703'), ('INFO', '2023-07-04 09:37:49,289 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:37:49,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff5e5347-c99f-4f3e-9c13-ed4a29e42634'), ('INFO', '2023-07-04 09:37:49,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4d527cd-f180-49bd-ae34-9f857bba1135'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO - Starting Worker plugin PreImport-b4ed644f-ce0b-440a-8f6b-111051085394'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-i4d1g5dq'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:37703'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43835'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:54895'), ('INFO', '2023-07-04 09:37:48,366 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:54895'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2349055)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 09:45:28,279 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2350175. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688456728.765783] [dgx-4:2350175:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 09:45:30,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:45:30,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-350' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 37213.52564 ms
Code block 'ddf-preprocessing' took: 15145.54860 ms
Code block 'ddf-preprocessing' took: 15012.44561 ms
Code block 'ddf-preprocessing' took: 15158.30226 ms
Code block 'ddf-preprocessing' took: 15031.42938 ms
Code block 'ddf-scaler-fit' took: 18530.12638 ms
Code block 'ddf-scaler-fit' took: 14121.61430 ms
Code block 'ddf-scaler-fit' took: 14358.26274 ms
Code block 'ddf-scaler-fit' took: 14818.99765 ms
Code block 'ddf-scaler-fit' took: 14990.51696 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45725.84649 ms
Code block 'ddf-kmeans-score' took: 15049.83337 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41977.60501 ms
Code block 'ddf-kmeans-score' took: 15141.73724 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41944.43409 ms
Code block 'ddf-kmeans-score' took: 15097.02849 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41891.16252 ms
Code block 'ddf-kmeans-score' took: 15047.34061 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42076.78141 ms
Code block 'ddf-kmeans-score' took: 14979.29356 ms
scores=[array(-4839461.65625)]
client.get_worker_logs()={'ucx://127.0.0.1:39651': (('INFO', "2023-07-04 09:53:07,029 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:52:39,805 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:52:08,846 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:51:41,610 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:51:10,809 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:50:43,535 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:50:12,665 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:49:45,486 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:49:14,407 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:48:43,585 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:45:31,782 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:45:31,782 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:54837'), ('INFO', '2023-07-04 09:45:31,703 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO - Starting Worker plugin PreImport-23e93df0-f37a-429a-81b4-f239545275c9'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e38481a3-5a49-4dd6-ab07-c67e7a933f8e'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27e8cf48-be36-4e71-9f89-1b519176b869'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-2bgni7i1'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:54837'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33211'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:39651'), ('INFO', '2023-07-04 09:45:30,783 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:39651'))}
+ sleep 1
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2357487)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 09:53:47,237 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2358592. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688457227.767700] [dgx-4:2358592:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 09:53:49,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 09:53:49,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-378' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39080.96920 ms
Code block 'ddf-preprocessing' took: 15389.50848 ms
Code block 'ddf-preprocessing' took: 15290.47130 ms
Code block 'ddf-preprocessing' took: 15355.99623 ms
Code block 'ddf-preprocessing' took: 15257.83900 ms
Code block 'ddf-scaler-fit' took: 16563.60084 ms
Code block 'ddf-scaler-fit' took: 12399.63237 ms
Code block 'ddf-scaler-fit' took: 12762.25517 ms
Code block 'ddf-scaler-fit' took: 13113.81113 ms
Code block 'ddf-scaler-fit' took: 13287.49566 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43155.78376 ms
Code block 'ddf-kmeans-score' took: 12289.45250 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39263.25195 ms
Code block 'ddf-kmeans-score' took: 12276.72235 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39262.72995 ms
Code block 'ddf-kmeans-score' took: 12565.79611 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39192.12758 ms
Code block 'ddf-kmeans-score' took: 12476.92497 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39247.60138 ms
Code block 'ddf-kmeans-score' took: 12308.95193 ms
scores=[array(-4838345.4140625)]
client.get_worker_logs()={'ucx://127.0.0.1:40959': (('INFO', "2023-07-04 10:00:56,236 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:00:29,053 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:00:03,468 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:59:36,300 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:59:10,654 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:58:43,501 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:58:18,067 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:57:50,879 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 09:57:25,405 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 09:56:54,374 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 09:53:51,117 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:53:51,117 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:47977'), ('INFO', '2023-07-04 09:53:51,043 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:53:50,107 - distributed.worker - INFO - Starting Worker plugin PreImport-59105b57-f54a-4424-bc4c-88d3ea2c3c89'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3bca7e62-1361-4daf-bbc5-3d48ebf1b781'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f5b9012-154a-4260-a9cc-3e1b7eab7fd1'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-kotbu5ii'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:47977'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42931'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:40959'), ('INFO', '2023-07-04 09:53:49,768 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:40959'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2366260)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 10:01:36,700 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2367347. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688457697.156409] [dgx-4:2367347:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:01:38,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:01:38,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-384' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39714.42037 ms
Code block 'ddf-preprocessing' took: 15417.59008 ms
Code block 'ddf-preprocessing' took: 15266.90588 ms
Code block 'ddf-preprocessing' took: 15408.05165 ms
Code block 'ddf-preprocessing' took: 15339.88689 ms
Code block 'ddf-scaler-fit' took: 19294.30722 ms
Code block 'ddf-scaler-fit' took: 15021.03777 ms
Code block 'ddf-scaler-fit' took: 15536.60058 ms
Code block 'ddf-scaler-fit' took: 15698.71084 ms
Code block 'ddf-scaler-fit' took: 16038.49870 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45762.18321 ms
Code block 'ddf-kmeans-score' took: 14885.56489 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41765.46598 ms
Code block 'ddf-kmeans-score' took: 14939.04098 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41880.41231 ms
Code block 'ddf-kmeans-score' took: 15140.39142 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41937.73398 ms
Code block 'ddf-kmeans-score' took: 15031.92981 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41824.46301 ms
Code block 'ddf-kmeans-score' took: 15087.82487 ms
scores=[array(-4837179.9140625)]
client.get_worker_logs()={'ucx://127.0.0.1:53335': (('INFO', "2023-07-04 10:09:23,623 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:08:56,455 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:08:25,716 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:07:58,518 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:07:27,590 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:07:00,397 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:06:29,704 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:06:02,534 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:05:31,716 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:05:00,891 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:01:40,575 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:01:40,574 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:44029'), ('INFO', '2023-07-04 10:01:40,501 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:01:40,501 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7213f435-8f1e-4baf-be86-65815ddbecdb'), ('INFO', '2023-07-04 10:01:39,423 - distributed.worker - INFO - Starting Worker plugin PreImport-5d4da176-7968-41d6-a315-c090bb24dfc5'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO - Starting Worker plugin RMMSetup-053ff7e0-358d-469a-9299-b0e5be078ed6'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-z6ac6v1n'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:44029'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40389'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:53335'), ('INFO', '2023-07-04 10:01:39,084 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:53335'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2377647)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 10:10:07,199 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2378747. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688458207.737201] [dgx-4:2378747:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:10:09,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:10:09,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-390' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 38308.83995 ms
Code block 'ddf-preprocessing' took: 15045.27949 ms
Code block 'ddf-preprocessing' took: 15200.48146 ms
Code block 'ddf-preprocessing' took: 15054.63471 ms
Code block 'ddf-preprocessing' took: 15149.93739 ms
Code block 'ddf-scaler-fit' took: 15339.58951 ms
Code block 'ddf-scaler-fit' took: 10841.68277 ms
Code block 'ddf-scaler-fit' took: 11031.60259 ms
Code block 'ddf-scaler-fit' took: 10946.46692 ms
Code block 'ddf-scaler-fit' took: 11146.54778 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41725.30727 ms
Code block 'ddf-kmeans-score' took: 10782.32290 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37776.82009 ms
Code block 'ddf-kmeans-score' took: 10872.39126 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37764.03291 ms
Code block 'ddf-kmeans-score' took: 10813.23440 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37783.44615 ms
Code block 'ddf-kmeans-score' took: 10977.78572 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37793.11584 ms
Code block 'ddf-kmeans-score' took: 10756.41642 ms
scores=[array(-4836852.0625)]
client.get_worker_logs()={'ucx://127.0.0.1:51641': (('INFO', "2023-07-04 10:16:52,317 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:16:25,151 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:16:02,541 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:15:35,366 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:15:12,939 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:14:45,789 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:14:23,295 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:13:56,158 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:13:33,549 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:13:02,710 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:10:11,231 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:10:11,231 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:60527'), ('INFO', '2023-07-04 10:10:11,156 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:10:10,210 - distributed.worker - INFO - Starting Worker plugin PreImport-4b17d805-a7c5-4ef3-b54d-e9bde020400e'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ea46fe1-381b-4056-b628-9043d8935150'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37daa7d6-e047-41ef-903c-98b4ea1693fe'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-2qki9yo2'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:60527'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40327'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:51641'), ('INFO', '2023-07-04 10:10:09,860 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:51641'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2385977)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 10:17:30,184 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2387075. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688458650.689796] [dgx-4:2387075:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:17:31,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:17:31,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-751' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 32650.48140 ms
Code block 'ddf-preprocessing' took: 15183.20353 ms
Code block 'ddf-preprocessing' took: 15080.66300 ms
Code block 'ddf-preprocessing' took: 15227.87409 ms
Code block 'ddf-preprocessing' took: 15014.77462 ms
Code block 'ddf-scaler-fit' took: 17375.66065 ms
Code block 'ddf-scaler-fit' took: 13494.22308 ms
Code block 'ddf-scaler-fit' took: 13542.56770 ms
Code block 'ddf-scaler-fit' took: 13721.69585 ms
Code block 'ddf-scaler-fit' took: 13848.53223 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44379.99820 ms
Code block 'ddf-kmeans-score' took: 13458.74682 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40307.35445 ms
Code block 'ddf-kmeans-score' took: 13450.38349 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40428.56533 ms
Code block 'ddf-kmeans-score' took: 13425.72281 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40691.87230 ms
Code block 'ddf-kmeans-score' took: 13516.12973 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40351.50025 ms
Code block 'ddf-kmeans-score' took: 13385.07813 ms
scores=[array(-4837191.65625)]
client.get_worker_logs()={'ucx://127.0.0.1:56559': (('INFO', "2023-07-04 10:24:51,175 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:24:24,081 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:23:56,293 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:23:29,129 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:23:01,166 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:22:33,988 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:22:06,262 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:21:39,102 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:21:11,423 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:20:40,546 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:17:39,385 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:17:39,385 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:55479'), ('INFO', '2023-07-04 10:17:39,301 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:17:33,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0000310f-dd56-412e-ac4c-27595a1e0c35'), ('INFO', '2023-07-04 10:17:32,431 - distributed.worker - INFO - Starting Worker plugin PreImport-9a37510f-897d-40b0-9d1e-b3bd740ce1ee'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e097092d-b755-48ab-aa58-426c00467856'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-sfgpz8ue'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:55479'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43845'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:56559'), ('INFO', '2023-07-04 10:17:32,430 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:56559'))}
+ sleep 1
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2395015)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 10:25:31,286 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2396084. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688459131.767394] [dgx-4:2396084:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:25:33,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:25:33,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-355' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 40004.38473 ms
Code block 'ddf-preprocessing' took: 15253.37984 ms
Code block 'ddf-preprocessing' took: 15359.45889 ms
Code block 'ddf-preprocessing' took: 15212.65049 ms
Code block 'ddf-preprocessing' took: 15242.47182 ms
Code block 'ddf-scaler-fit' took: 17860.05132 ms
Code block 'ddf-scaler-fit' took: 13961.72817 ms
Code block 'ddf-scaler-fit' took: 14678.51119 ms
Code block 'ddf-scaler-fit' took: 15119.67586 ms
Code block 'ddf-scaler-fit' took: 15933.31417 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45551.88811 ms
Code block 'ddf-kmeans-score' took: 15343.58317 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41819.97939 ms
Code block 'ddf-kmeans-score' took: 15384.99275 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42029.42915 ms
Code block 'ddf-kmeans-score' took: 15159.52064 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42060.21249 ms
Code block 'ddf-kmeans-score' took: 15705.07255 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41945.36960 ms
Code block 'ddf-kmeans-score' took: 15706.70005 ms
scores=[array(-4838100.8203125)]
client.get_worker_logs()={'ucx://127.0.0.1:60777': (('INFO', "2023-07-04 10:33:15,835 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:32:48,546 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:32:17,099 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:31:49,820 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:31:18,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:30:51,501 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:30:20,258 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:29:53,088 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:29:21,834 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:28:51,063 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:25:34,796 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:25:34,796 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:41047'), ('INFO', '2023-07-04 10:25:34,719 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:25:33,800 - distributed.worker - INFO - Starting Worker plugin PreImport-22f8f509-f319-447f-944f-d34eebdbbe52'), ('INFO', '2023-07-04 10:25:33,800 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0e65cee-688b-4cf5-8ab5-00ab473e32bf'), ('INFO', '2023-07-04 10:25:33,800 - distributed.worker - INFO - Starting Worker plugin RMMSetup-801638f2-7ef8-4cc9-b975-8e7b23510dfa'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-_33we4g5'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:41047'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39997'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:60777'), ('INFO', '2023-07-04 10:25:33,799 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:60777'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2405958)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 10:34:00,271 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2407311. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688459640.713380] [dgx-4:2407311:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:34:02,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:34:02,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-354' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 37793.28693 ms
Code block 'ddf-preprocessing' took: 15300.01322 ms
Code block 'ddf-preprocessing' took: 15419.40730 ms
Code block 'ddf-preprocessing' took: 15316.01639 ms
Code block 'ddf-preprocessing' took: 15447.96840 ms
Code block 'ddf-scaler-fit' took: 20616.39464 ms
Code block 'ddf-scaler-fit' took: 16846.77875 ms
Code block 'ddf-scaler-fit' took: 17335.54824 ms
Code block 'ddf-scaler-fit' took: 17952.06980 ms
Code block 'ddf-scaler-fit' took: 18491.34227 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 48705.21500 ms
Code block 'ddf-kmeans-score' took: 18126.43135 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45089.45910 ms
Code block 'ddf-kmeans-score' took: 18328.16470 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44936.49685 ms
Code block 'ddf-kmeans-score' took: 18245.64896 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45141.69029 ms
Code block 'ddf-kmeans-score' took: 18276.02698 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44913.51067 ms
Code block 'ddf-kmeans-score' took: 18381.64707 ms
scores=[array(-4838627.828125)]
client.get_worker_logs()={'ucx://127.0.0.1:41801': (('INFO', "2023-07-04 10:42:23,376 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:41:56,146 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:41:19,074 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:40:51,799 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:40:14,586 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:39:47,335 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:39:10,215 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:38:42,989 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:38:05,740 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:37:34,776 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:34:03,733 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:34:03,733 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:53431'), ('INFO', '2023-07-04 10:34:03,653 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:34:03,653 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bce1bc7f-8e69-4400-b209-c386513b478b'), ('INFO', '2023-07-04 10:34:02,741 - distributed.worker - INFO - Starting Worker plugin PreImport-c1cbffe5-6f50-4ca6-94c4-09071d17eb57'), ('INFO', '2023-07-04 10:34:02,741 - distributed.worker - INFO - Starting Worker plugin RMMSetup-886a0947-2c75-4611-9efe-800c016c4deb'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-yd8fwmuj'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:53431'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40091'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:41801'), ('INFO', '2023-07-04 10:34:02,740 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:41801'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2416348)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 10:43:08,581 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2417405. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688460189.023211] [dgx-4:2417405:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:43:10,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:43:10,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-352' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39133.55597 ms
Code block 'ddf-preprocessing' took: 15044.68982 ms
Code block 'ddf-preprocessing' took: 15189.48311 ms
Code block 'ddf-preprocessing' took: 15080.36667 ms
Code block 'ddf-preprocessing' took: 15091.30286 ms
Code block 'ddf-scaler-fit' took: 16168.67713 ms
Code block 'ddf-scaler-fit' took: 11636.40306 ms
Code block 'ddf-scaler-fit' took: 12118.54208 ms
Code block 'ddf-scaler-fit' took: 12177.52391 ms
Code block 'ddf-scaler-fit' took: 12271.12706 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43081.45479 ms
Code block 'ddf-kmeans-score' took: 12323.62959 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39305.57760 ms
Code block 'ddf-kmeans-score' took: 12373.24174 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39279.54153 ms
Code block 'ddf-kmeans-score' took: 12361.15769 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39446.18633 ms
Code block 'ddf-kmeans-score' took: 14501.52895 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39252.19376 ms
2023-07-04 10:50:14,830 - distributed.protocol.core - CRITICAL - Failed to Serialize
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/core.py", line 109, in dumps
    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/msgpack/__init__.py", line 38, in packb
    return Packer(**kwargs).pack(o)
  File "msgpack/_packer.pyx", line 294, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 300, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 297, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  [Previous line repeated 508 more times]
  File "msgpack/_packer.pyx", line 229, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 163, in msgpack._cmsgpack.Packer._pack
ValueError: recursion limit exceeded.
2023-07-04 10:50:14,835 - distributed.comm.utils - ERROR - recursion limit exceeded.
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/utils.py", line 55, in _to_frames
    return list(protocol.dumps(msg, **kwargs))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/core.py", line 109, in dumps
    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/msgpack/__init__.py", line 38, in packb
    return Packer(**kwargs).pack(o)
  File "msgpack/_packer.pyx", line 294, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 300, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 297, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  [Previous line repeated 508 more times]
  File "msgpack/_packer.pyx", line 229, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 163, in msgpack._cmsgpack.Packer._pack
ValueError: recursion limit exceeded.
2023-07-04 10:50:15,017 - distributed.core - ERROR - recursion limit exceeded.
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/ucx.py", line 294, in write
    frames = await to_frames(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/utils.py", line 70, in to_frames
    return await offload(_to_frames)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1417, in offload
    return await loop.run_in_executor(
  File "/opt/conda/envs/rapids/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1418, in <lambda>
    _offload_executor, lambda: context.run(fn, *args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/utils.py", line 55, in _to_frames
    return list(protocol.dumps(msg, **kwargs))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/core.py", line 109, in dumps
    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/msgpack/__init__.py", line 38, in packb
    return Packer(**kwargs).pack(o)
  File "msgpack/_packer.pyx", line 294, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 300, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 297, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  [Previous line repeated 508 more times]
  File "msgpack/_packer.pyx", line 229, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 163, in msgpack._cmsgpack.Packer._pack
ValueError: recursion limit exceeded.
Task exception was never retrieved
future: <Task finished name='Task-43014' coro=<Server._handle_comm() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py:726> exception=ValueError('recursion limit exceeded.')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 838, in _handle_comm
    await comm.write(result, serializers=serializers)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/ucx.py", line 294, in write
    frames = await to_frames(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/utils.py", line 70, in to_frames
    return await offload(_to_frames)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1417, in offload
    return await loop.run_in_executor(
  File "/opt/conda/envs/rapids/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1418, in <lambda>
    _offload_executor, lambda: context.run(fn, *args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/utils.py", line 55, in _to_frames
    return list(protocol.dumps(msg, **kwargs))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/protocol/core.py", line 109, in dumps
    frames[0] = msgpack.dumps(msg, default=_encode_default, use_bin_type=True)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/msgpack/__init__.py", line 38, in packb
    return Packer(**kwargs).pack(o)
  File "msgpack/_packer.pyx", line 294, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 300, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 297, in msgpack._cmsgpack.Packer.pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 231, in msgpack._cmsgpack.Packer._pack
  [Previous line repeated 508 more times]
  File "msgpack/_packer.pyx", line 229, in msgpack._cmsgpack.Packer._pack
  File "msgpack/_packer.pyx", line 163, in msgpack._cmsgpack.Packer._pack
ValueError: recursion limit exceeded.
Code block 'ddf-kmeans-score' took: 12483.47773 ms
scores=[array(-4838676.03125)]
client.get_worker_logs()={'ucx://127.0.0.1:52313': (('INFO', "2023-07-04 10:50:14,733 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:49:47,548 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:49:19,950 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:48:52,734 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:48:27,143 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:47:59,943 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:47:34,340 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:47:07,169 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:46:41,521 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:46:10,786 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:43:12,021 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:43:12,021 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:40973'), ('INFO', '2023-07-04 10:43:11,943 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:43:11,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28bafd22-f334-496f-90cd-7342fb4bcf61'), ('INFO', '2023-07-04 10:43:11,943 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7fccbc3-41a2-4ca9-a30d-ccb582f5e95b'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO - Starting Worker plugin PreImport-f91b4b0f-ec11-4fb9-b8b4-779fa6afe0c9'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-8_9bv1hx'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:40973'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44881'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:52313'), ('INFO', '2023-07-04 10:43:11,025 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:52313'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2425187)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 10:50:56,666 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2426256. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688460657.124951] [dgx-4:2426256:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:50:58,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:50:58,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-351' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39971.75142 ms
Code block 'ddf-preprocessing' took: 15080.30941 ms
Code block 'ddf-preprocessing' took: 15158.14889 ms
Code block 'ddf-preprocessing' took: 15136.90976 ms
Code block 'ddf-preprocessing' took: 15126.27548 ms
Code block 'ddf-scaler-fit' took: 18495.94650 ms
Code block 'ddf-scaler-fit' took: 14340.53559 ms
Code block 'ddf-scaler-fit' took: 14623.60228 ms
Code block 'ddf-scaler-fit' took: 14899.57722 ms
Code block 'ddf-scaler-fit' took: 14976.81578 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45762.99677 ms
Code block 'ddf-kmeans-score' took: 14941.07489 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41892.54886 ms
Code block 'ddf-kmeans-score' took: 15192.25426 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41926.19547 ms
Code block 'ddf-kmeans-score' took: 15021.00790 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42006.27192 ms
Code block 'ddf-kmeans-score' took: 15054.16348 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41978.44617 ms
Code block 'ddf-kmeans-score' took: 15071.02858 ms
scores=[array(-4835408.5625)]
client.get_worker_logs()={'ucx://127.0.0.1:44773': (('INFO', "2023-07-04 10:58:38,647 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:58:11,429 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:57:40,560 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:57:13,331 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:56:42,480 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:56:15,247 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:55:44,317 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:55:17,135 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 10:54:46,394 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 10:54:15,428 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:51:00,116 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:51:00,116 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:45087'), ('INFO', '2023-07-04 10:51:00,038 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:51:00,038 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-332fd1b7-cfd4-4a64-a0e8-2e463e110f84'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO - Starting Worker plugin PreImport-747dc98d-bb24-401b-8257-d027baccda81'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d9f9e81-414f-405d-bcdb-0f6a75be935c'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ir4jor38'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:45087'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34277'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:50:59,125 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:44773'), ('INFO', '2023-07-04 10:50:59,124 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:44773'))}
+ sleep 1
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2434255)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 10:59:20,665 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2435314. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688461161.117392] [dgx-4:2435314:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 10:59:22,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 10:59:22,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-352' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39485.87045 ms
Code block 'ddf-preprocessing' took: 15282.52929 ms
Code block 'ddf-preprocessing' took: 15338.57204 ms
Code block 'ddf-preprocessing' took: 15226.06521 ms
Code block 'ddf-preprocessing' took: 15420.30410 ms
Code block 'ddf-scaler-fit' took: 16498.73459 ms
Code block 'ddf-scaler-fit' took: 12486.98571 ms
Code block 'ddf-scaler-fit' took: 12636.01999 ms
Code block 'ddf-scaler-fit' took: 13133.90552 ms
Code block 'ddf-scaler-fit' took: 13211.79831 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43015.11036 ms
Code block 'ddf-kmeans-score' took: 12320.44305 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39125.92206 ms
Code block 'ddf-kmeans-score' took: 12251.98405 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39263.82194 ms
Code block 'ddf-kmeans-score' took: 12340.51430 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39343.22371 ms
Code block 'ddf-kmeans-score' took: 12262.71740 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39221.32481 ms
Code block 'ddf-kmeans-score' took: 12474.43157 ms
scores=[array(-4836772.9140625)]
client.get_worker_logs()={'ucx://127.0.0.1:44799': (('INFO', "2023-07-04 11:06:29,375 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:06:02,098 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:05:36,708 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:05:09,520 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:04:43,961 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:04:16,718 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:03:51,383 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:03:24,131 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:02:58,600 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:02:27,719 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 10:59:24,156 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:59:24,156 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:34067'), ('INFO', '2023-07-04 10:59:24,076 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:59:23,146 - distributed.worker - INFO - Starting Worker plugin PreImport-561ffe00-ad39-4f8d-9bfc-20f51f129962'), ('INFO', '2023-07-04 10:59:23,146 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13173107-0690-4e0e-8a81-30209654168a'), ('INFO', '2023-07-04 10:59:22,845 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9c02082-230a-4960-a584-fb270fd373f1'), ('INFO', '2023-07-04 10:59:22,845 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-i29spiu8'), ('INFO', '2023-07-04 10:59:22,845 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 10:59:22,845 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:34067'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37691'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:44799'), ('INFO', '2023-07-04 10:59:22,844 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:44799'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2442978)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 11:07:09,356 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2444077. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688461629.790182] [dgx-4:2444077:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:07:11,348 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:07:11,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-790' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 32997.73753 ms
Code block 'ddf-preprocessing' took: 15372.61806 ms
Code block 'ddf-preprocessing' took: 15290.35951 ms
Code block 'ddf-preprocessing' took: 15407.79612 ms
Code block 'ddf-preprocessing' took: 15274.16486 ms
Code block 'ddf-scaler-fit' took: 18854.72667 ms
Code block 'ddf-scaler-fit' took: 16185.86941 ms
Code block 'ddf-scaler-fit' took: 17133.17676 ms
Code block 'ddf-scaler-fit' took: 16453.53155 ms
Code block 'ddf-scaler-fit' took: 16871.85885 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46657.39949 ms
Code block 'ddf-kmeans-score' took: 15881.84533 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43071.14162 ms
Code block 'ddf-kmeans-score' took: 16026.01873 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43224.09450 ms
Code block 'ddf-kmeans-score' took: 16112.55664 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42968.60584 ms
Code block 'ddf-kmeans-score' took: 16123.94590 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43172.47381 ms
Code block 'ddf-kmeans-score' took: 15566.22621 ms
scores=[array(-4838316.3359375)]
client.get_worker_logs()={'ucx://127.0.0.1:42791': (('INFO', "2023-07-04 11:15:09,108 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:14:41,944 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:14:08,777 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:13:41,590 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:13:08,645 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:12:41,287 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:12:08,322 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:11:41,128 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:11:08,140 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:10:37,125 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:07:19,107 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:07:19,107 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:51913'), ('INFO', '2023-07-04 11:07:19,019 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:07:12,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37e4df00-a6cb-46f6-9a4b-5c375a9cff7e'), ('INFO', '2023-07-04 11:07:12,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70728c00-c28f-4de1-9d88-22eecb4589f6'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO - Starting Worker plugin PreImport-a0f0e0d8-1b13-4362-acb1-cffc62336a7e'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ah8crs8i'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:51913'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40901'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:42791'), ('INFO', '2023-07-04 11:07:11,817 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:42791'))}
2023-07-04 11:15:29,450 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2453821)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 11:15:53,888 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2454898. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688462154.360314] [dgx-4:2454898:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:15:55,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:15:55,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 40012.72150 ms
Code block 'ddf-preprocessing' took: 14982.95156 ms
Code block 'ddf-preprocessing' took: 14777.51161 ms
Code block 'ddf-preprocessing' took: 14762.68703 ms
Code block 'ddf-preprocessing' took: 14980.12814 ms
Code block 'ddf-scaler-fit' took: 16097.44655 ms
Code block 'ddf-scaler-fit' took: 11540.71333 ms
Code block 'ddf-scaler-fit' took: 11540.98881 ms
Code block 'ddf-scaler-fit' took: 12264.42072 ms
Task exception was never retrieved
future: <Task finished name='Task-390' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-scaler-fit' took: 12518.83978 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43198.06180 ms
Code block 'ddf-kmeans-score' took: 12011.18947 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 38989.31846 ms
Code block 'ddf-kmeans-score' took: 11626.72918 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39196.10677 ms
Code block 'ddf-kmeans-score' took: 11589.00565 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 38448.34169 ms
Code block 'ddf-kmeans-score' took: 11511.33645 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 38506.30605 ms
Code block 'ddf-kmeans-score' took: 11866.87282 ms
scores=[array(-4837422.59375)]
client.get_worker_logs()={'ucx://127.0.0.1:50239': (('INFO', "2023-07-04 11:22:53,094 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:22:25,886 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:22:02,067 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:21:34,853 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:21:11,011 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:20:43,804 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:20:19,153 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:19:51,948 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:19:27,081 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:18:56,042 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:15:57,902 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:15:57,901 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:35207'), ('INFO', '2023-07-04 11:15:57,826 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:15:56,873 - distributed.worker - INFO - Starting Worker plugin PreImport-e3162f59-d839-4577-903f-fca632aeb264'), ('INFO', '2023-07-04 11:15:56,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b8258ca-5d8e-486f-92c7-32bf02205ed7'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fd5cee9-c24d-4f54-9bad-5dcde5ce5d87'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-pkdhxd8r'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:35207'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40289'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:50239'), ('INFO', '2023-07-04 11:15:56,486 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:50239'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=--enable-infiniband
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2462819)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-infiniband --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=True, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 11:23:34,412 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2463901. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688462614.916710] [dgx-4:2463901:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:23:36,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:23:36,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 38730.04316 ms
Code block 'ddf-preprocessing' took: 14682.78229 ms
Code block 'ddf-preprocessing' took: 15018.06634 ms
Code block 'ddf-preprocessing' took: 14783.70026 ms
Code block 'ddf-preprocessing' took: 14683.16141 ms
Code block 'ddf-scaler-fit' took: 18359.98334 ms
Code block 'ddf-scaler-fit' took: 14847.33924 ms
Code block 'ddf-scaler-fit' took: 14857.31506 ms
Code block 'ddf-scaler-fit' took: 15057.84375 ms
Task exception was never retrieved
future: <Task finished name='Task-389' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-scaler-fit' took: 14544.59067 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46382.68033 ms
Code block 'ddf-kmeans-score' took: 14171.19367 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41024.46516 ms
Code block 'ddf-kmeans-score' took: 14758.81595 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41513.42780 ms
Code block 'ddf-kmeans-score' took: 14824.04378 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41566.69398 ms
Code block 'ddf-kmeans-score' took: 15032.04697 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41423.76297 ms
Code block 'ddf-kmeans-score' took: 13908.87752 ms
scores=[array(-4836840.78125)]
client.get_worker_logs()={'ucx://127.0.0.1:41405': (('INFO', "2023-07-04 11:31:11,417 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:30:44,275 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:30:13,939 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:29:46,810 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:29:16,506 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:28:49,386 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:28:19,193 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:27:52,035 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:27:22,953 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:26:51,578 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:23:38,493 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:23:38,492 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:53301'), ('INFO', '2023-07-04 11:23:38,414 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:23:38,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31b25afe-b29e-42ef-81f9-c1d59ceba624'), ('INFO', '2023-07-04 11:23:37,359 - distributed.worker - INFO - Starting Worker plugin PreImport-0c0689a8-2c23-4d16-af0d-d806ba0e36ac'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO - Starting Worker plugin RMMSetup-266dab35-4ede-4669-a31c-b2dcfceac9ff'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-rz2vgfvq'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:53301'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44851'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:41405'), ('INFO', '2023-07-04 11:23:36,984 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:41405'))}
2023-07-04 11:31:29,912 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing
+ sleep 1
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2472666)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 11:31:54,350 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2473762. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688463114.832621] [dgx-4:2473762:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:31:56,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:31:56,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-322' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 40605.37963 ms
Code block 'ddf-preprocessing' took: 15000.89700 ms
Code block 'ddf-preprocessing' took: 15228.37278 ms
Code block 'ddf-preprocessing' took: 15077.96039 ms
Code block 'ddf-preprocessing' took: 15003.57020 ms
Code block 'ddf-scaler-fit' took: 18702.76954 ms
Code block 'ddf-scaler-fit' took: 14779.67546 ms
Code block 'ddf-scaler-fit' took: 16105.37556 ms
Code block 'ddf-scaler-fit' took: 16525.98465 ms
Code block 'ddf-scaler-fit' took: 16652.94740 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46839.34239 ms
Code block 'ddf-kmeans-score' took: 16346.90555 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43558.51167 ms
Code block 'ddf-kmeans-score' took: 16765.44669 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43445.63894 ms
Code block 'ddf-kmeans-score' took: 16562.27082 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43649.27529 ms
Code block 'ddf-kmeans-score' took: 17195.83925 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43074.89619 ms
Code block 'ddf-kmeans-score' took: 16561.20365 ms
scores=[array(-4838430.5625)]
client.get_worker_logs()={'ucx://127.0.0.1:46999': (('INFO', "2023-07-04 11:39:56,326 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:39:29,105 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:38:54,954 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:38:27,613 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:37:53,635 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:37:26,416 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:36:52,335 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:36:25,055 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:35:51,159 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:35:20,033 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:31:57,740 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:31:57,739 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:60085'), ('INFO', '2023-07-04 11:31:57,661 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO - Starting Worker plugin PreImport-86b7cc26-e685-42ac-900e-46d10e981065'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf295c80-5459-45d9-baf5-0d5b225b081c'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee375ce7-7c32-45d4-af6b-44b625a589a6'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-wfme8d4y'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:60085'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37553'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:46999'), ('INFO', '2023-07-04 11:31:56,716 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:46999'))}
2023-07-04 11:40:17,736 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2483355)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 11:40:43,032 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2484436. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688463643.380494] [dgx-4:2484436:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:40:44,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:40:44,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-333' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39053.98693 ms
Code block 'ddf-preprocessing' took: 15204.56816 ms
Code block 'ddf-preprocessing' took: 15107.93749 ms
Code block 'ddf-preprocessing' took: 14849.11410 ms
Code block 'ddf-preprocessing' took: 14974.67144 ms
Code block 'ddf-scaler-fit' took: 21233.56633 ms
Code block 'ddf-scaler-fit' took: 19831.23601 ms
Code block 'ddf-scaler-fit' took: 18242.97538 ms
Code block 'ddf-scaler-fit' took: 18878.61634 ms
Code block 'ddf-scaler-fit' took: 19570.62071 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 50124.42893 ms
Code block 'ddf-kmeans-score' took: 19244.51243 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46367.65559 ms
Code block 'ddf-kmeans-score' took: 19498.28416 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46224.69139 ms
Code block 'ddf-kmeans-score' took: 19841.50615 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46837.34248 ms
Code block 'ddf-kmeans-score' took: 19731.43700 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46398.95985 ms
Code block 'ddf-kmeans-score' took: 22899.35299 ms
scores=[array(-4837148.5859375)]
client.get_worker_logs()={'ucx://127.0.0.1:53675': (('INFO', "2023-07-04 11:49:25,309 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:48:58,088 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:48:18,067 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:47:50,768 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:47:10,264 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:46:42,989 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:46:03,396 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:45:36,156 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:44:56,478 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:44:25,221 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:40:46,398 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:40:46,398 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:59363'), ('INFO', '2023-07-04 11:40:46,271 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:40:46,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7fd1ae4f-332e-44ab-8135-2a10b304b4f9'), ('INFO', '2023-07-04 11:40:46,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65c63270-9396-4213-925c-fd711a760508'), ('INFO', '2023-07-04 11:40:45,337 - distributed.worker - INFO - Starting Worker plugin PreImport-b829af18-446a-4659-9ad0-95c828750894'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-xbg_3plj'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:59363'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44501'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:53675'), ('INFO', '2023-07-04 11:40:45,336 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:53675'))}
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2493819)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 11:50:15,379 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2494933. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688464215.677736] [dgx-4:2494933:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:50:16,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:50:16,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-307' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39999.89764 ms
Code block 'ddf-preprocessing' took: 15076.81351 ms
Code block 'ddf-preprocessing' took: 15222.87505 ms
Code block 'ddf-preprocessing' took: 15032.26365 ms
Code block 'ddf-preprocessing' took: 15117.04066 ms
Code block 'ddf-scaler-fit' took: 15928.12356 ms
Code block 'ddf-scaler-fit' took: 11664.30978 ms
Code block 'ddf-scaler-fit' took: 11868.28023 ms
Code block 'ddf-scaler-fit' took: 12093.92074 ms
Code block 'ddf-scaler-fit' took: 12338.85598 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42935.57216 ms
Code block 'ddf-kmeans-score' took: 12434.50618 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39199.38298 ms
Code block 'ddf-kmeans-score' took: 12352.98031 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39361.96596 ms
Code block 'ddf-kmeans-score' took: 12446.65744 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39258.16512 ms
Code block 'ddf-kmeans-score' took: 12338.62953 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39309.91318 ms
Code block 'ddf-kmeans-score' took: 12407.28474 ms
scores=[array(-4838669.21875)]
client.get_worker_logs()={'ucx://127.0.0.1:36293': (('INFO', "2023-07-04 11:57:19,428 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:56:52,266 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:56:26,758 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:55:59,552 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:55:34,010 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:55:06,840 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:54:41,112 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:54:13,982 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 11:53:48,373 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 11:53:17,629 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:50:18,360 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:50:18,360 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:40671'), ('INFO', '2023-07-04 11:50:18,266 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO - Starting Worker plugin PreImport-75517178-b8aa-46e6-b711-4752ccdcf7bb'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0279004f-5e22-4351-ad5b-b42b48c9525e'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a02c49c8-7b95-43fa-b143-575eef712649'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-0yg58edo'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:40671'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44159'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:36293'), ('INFO', '2023-07-04 11:50:17,348 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:36293'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2502005)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
/opt/conda/envs/rapids/lib/python3.10/site-packages/dask_cuda-23.2.0-py3.10.egg/dask_cuda/local_cuda_cluster.py:266: UserWarning: When using NVLink we recommend setting a `rmm_pool_size`. Please see: https://dask-cuda.readthedocs.io/en/latest/ucx.html#important-notes for more details
2023-07-04 11:58:00,719 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2503097. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688464681.115036] [dgx-4:2503097:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 11:58:02,667 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 11:58:02,668 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-328' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 38748.20805 ms
Code block 'ddf-preprocessing' took: 15289.07181 ms
Code block 'ddf-preprocessing' took: 15305.97040 ms
Code block 'ddf-preprocessing' took: 15405.64220 ms
Code block 'ddf-preprocessing' took: 15086.95129 ms
Code block 'ddf-scaler-fit' took: 18648.42026 ms
Code block 'ddf-scaler-fit' took: 14216.89510 ms
Code block 'ddf-scaler-fit' took: 14525.03479 ms
Code block 'ddf-scaler-fit' took: 14871.57626 ms
Code block 'ddf-scaler-fit' took: 14916.37837 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45742.61360 ms
Code block 'ddf-kmeans-score' took: 15076.66768 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41942.21136 ms
Code block 'ddf-kmeans-score' took: 15148.09216 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42071.12907 ms
Code block 'ddf-kmeans-score' took: 15044.62513 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41934.87190 ms
Code block 'ddf-kmeans-score' took: 16533.80215 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42903.88734 ms
Code block 'ddf-kmeans-score' took: 15682.13993 ms
scores=[array(-4836854.71875)]
client.get_worker_logs()={'ucx://127.0.0.1:38767': (('INFO', "2023-07-04 12:05:44,514 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:05:17,256 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:04:44,022 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:04:16,799 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:03:45,972 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:03:18,664 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:02:47,707 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:02:20,475 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:01:49,463 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:01:18,645 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 11:58:04,055 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:58:04,055 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:49935'), ('INFO', '2023-07-04 11:58:03,977 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:58:03,977 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16e5edeb-4a57-478a-ae1e-97dfdfc49d41'), ('INFO', '2023-07-04 11:58:03,977 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4cb5b29-86e5-4853-ab4e-9bee49b022de'), ('INFO', '2023-07-04 11:58:03,064 - distributed.worker - INFO - Starting Worker plugin PreImport-8522c854-bdbd-45de-9e9f-d27e253276d4'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ofp0k85r'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:49935'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37225'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:38767'), ('INFO', '2023-07-04 11:58:03,063 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:38767'))}
+ sleep 1
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2511707)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 12:06:27,979 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2512801. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688465188.441215] [dgx-4:2512801:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:06:29,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:06:29,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-352' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 40256.65922 ms
Code block 'ddf-preprocessing' took: 14901.17840 ms
Code block 'ddf-preprocessing' took: 15022.06457 ms
Code block 'ddf-preprocessing' took: 15189.17365 ms
Code block 'ddf-preprocessing' took: 14991.05722 ms
Code block 'ddf-scaler-fit' took: 17187.56942 ms
Code block 'ddf-scaler-fit' took: 13525.87066 ms
Code block 'ddf-scaler-fit' took: 21084.44590 ms
Code block 'ddf-scaler-fit' took: 14767.61346 ms
Code block 'ddf-scaler-fit' took: 14066.30797 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44993.69216 ms
Code block 'ddf-kmeans-score' took: 13506.45362 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39774.00560 ms
Code block 'ddf-kmeans-score' took: 13577.18646 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40648.10578 ms
Code block 'ddf-kmeans-score' took: 13500.54615 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40063.45871 ms
Code block 'ddf-kmeans-score' took: 13588.47031 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40185.35539 ms
Code block 'ddf-kmeans-score' took: 13604.50558 ms
scores=[array(-4837281.25)]
client.get_worker_logs()={'ucx://127.0.0.1:39951': (('INFO', "2023-07-04 12:13:59,527 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:13:32,346 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:13:04,696 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:12:37,528 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:12:10,062 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:11:42,844 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:11:14,788 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:10:47,516 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:10:20,421 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:09:49,062 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:06:31,763 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:06:31,762 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:36413'), ('INFO', '2023-07-04 12:06:31,694 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:06:30,753 - distributed.worker - INFO - Starting Worker plugin PreImport-30dfcac9-0baa-4d78-abf8-a7ff4ce5c6d9'), ('INFO', '2023-07-04 12:06:30,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d87b42af-5bc1-4743-b0a9-1445ea460f13'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a232c42d-3eed-405a-8423-2a6ad52ccb07'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-_rfia_vk'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:36413'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44205'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:39951'), ('INFO', '2023-07-04 12:06:30,401 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:39951'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2521788)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 12:14:41,795 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2522865. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688465682.123104] [dgx-4:2522865:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:14:43,670 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:14:43,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-346' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39727.35996 ms
Code block 'ddf-preprocessing' took: 14993.90039 ms
Code block 'ddf-preprocessing' took: 15060.06403 ms
Code block 'ddf-preprocessing' took: 15356.81469 ms
Code block 'ddf-preprocessing' took: 15094.45606 ms
Code block 'ddf-scaler-fit' took: 19609.61718 ms
Code block 'ddf-scaler-fit' took: 15488.99901 ms
Code block 'ddf-scaler-fit' took: 16869.66753 ms
Code block 'ddf-scaler-fit' took: 17003.82543 ms
Code block 'ddf-scaler-fit' took: 17538.33164 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 47403.04764 ms
Code block 'ddf-kmeans-score' took: 15885.24937 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43065.19647 ms
Code block 'ddf-kmeans-score' took: 16177.81189 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43216.50025 ms
Code block 'ddf-kmeans-score' took: 16276.01911 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43116.79442 ms
Code block 'ddf-kmeans-score' took: 16110.66686 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42633.73695 ms
Code block 'ddf-kmeans-score' took: 16170.35697 ms
scores=[array(-4838214.9296875)]
client.get_worker_logs()={'ucx://127.0.0.1:39805': (('INFO', "2023-07-04 12:22:42,858 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:22:15,612 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:21:43,049 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:21:15,780 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:20:42,629 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:20:15,380 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:19:42,193 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:19:14,972 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:18:42,163 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:18:10,826 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:14:45,416 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:14:45,416 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:37115'), ('INFO', '2023-07-04 12:14:45,348 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:14:44,417 - distributed.worker - INFO - Starting Worker plugin PreImport-9997038e-31ed-4eac-9b0b-1138026f6e47'), ('INFO', '2023-07-04 12:14:44,417 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6075c0bb-ad55-467f-be37-9f0bdf6bc673'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33a8eb84-d2df-4221-9b6f-5cea9238d655'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-p0x3thz4'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:37115'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46235'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:39805'), ('INFO', '2023-07-04 12:14:44,090 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:39805'))}
2023-07-04 12:23:03,951 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2531824)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 12:23:28,786 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2532915. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688466209.124170] [dgx-4:2532915:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:23:30,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:23:30,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-347' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 40197.93555 ms
Code block 'ddf-preprocessing' took: 14828.00427 ms
Code block 'ddf-preprocessing' took: 14790.98292 ms
Code block 'ddf-preprocessing' took: 15018.76714 ms
Code block 'ddf-preprocessing' took: 14995.20123 ms
Code block 'ddf-scaler-fit' took: 15855.73265 ms
Code block 'ddf-scaler-fit' took: 11177.39863 ms
Code block 'ddf-scaler-fit' took: 11882.17191 ms
Code block 'ddf-scaler-fit' took: 12069.58486 ms
Code block 'ddf-scaler-fit' took: 12026.41995 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42515.33705 ms
Code block 'ddf-kmeans-score' took: 12066.57385 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39111.22367 ms
Code block 'ddf-kmeans-score' took: 11303.88917 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 38442.35491 ms
Code block 'ddf-kmeans-score' took: 12001.09849 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39243.68041 ms
Code block 'ddf-kmeans-score' took: 11991.59788 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39039.03561 ms
Code block 'ddf-kmeans-score' took: 11725.08266 ms
scores=[array(-4836161.5625)]
client.get_worker_logs()={'ucx://127.0.0.1:43603': (('INFO', "2023-07-04 12:30:27,914 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:30:00,698 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:29:35,858 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:29:08,669 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:28:43,594 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:28:16,411 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:27:52,833 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:27:25,644 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:27:00,379 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:26:29,365 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:23:32,406 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:23:32,405 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:60173'), ('INFO', '2023-07-04 12:23:32,336 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:23:31,392 - distributed.worker - INFO - Starting Worker plugin PreImport-364554f2-2bae-4db4-af37-562c8608979e'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2402ee72-2d9f-4926-ba23-e3f4269dedd9'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96985502-a607-45b4-aeb1-53ac0ee01640'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-ptuu24e6'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:60173'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44369'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:43603'), ('INFO', '2023-07-04 12:23:31,079 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:43603'))}
2023-07-04 12:30:44,218 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=--enable-nvlink
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2541452)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --enable-nvlink --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=True, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 12:31:09,416 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2542531. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688466669.786941] [dgx-4:2542531:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:31:11,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:31:11,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-358' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39817.41958 ms
Code block 'ddf-preprocessing' took: 14858.56529 ms
Code block 'ddf-preprocessing' took: 15128.64408 ms
Code block 'ddf-preprocessing' took: 14940.85938 ms
Code block 'ddf-preprocessing' took: 14935.12212 ms
Code block 'ddf-scaler-fit' took: 18702.55094 ms
Code block 'ddf-scaler-fit' took: 15213.48678 ms
Code block 'ddf-scaler-fit' took: 14280.17209 ms
Code block 'ddf-scaler-fit' took: 14456.72775 ms
Code block 'ddf-scaler-fit' took: 14369.91121 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46489.68996 ms
Code block 'ddf-kmeans-score' took: 14032.90198 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41183.48433 ms
Code block 'ddf-kmeans-score' took: 14994.76666 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41193.05270 ms
Code block 'ddf-kmeans-score' took: 15063.46522 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41990.60802 ms
Code block 'ddf-kmeans-score' took: 14467.18529 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42105.18650 ms
Code block 'ddf-kmeans-score' took: 14389.22643 ms
scores=[array(-4838255.15625)]
client.get_worker_logs()={'ucx://127.0.0.1:39165': (('INFO', "2023-07-04 12:38:48,469 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:38:21,325 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:37:50,826 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:37:23,718 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:36:52,722 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:36:25,533 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:35:55,491 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:35:28,261 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:34:59,025 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:34:27,767 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:31:13,161 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:31:13,161 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:47569'), ('INFO', '2023-07-04 12:31:13,094 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:31:13,094 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d64f940a-ec43-49fc-bb7d-019e757e8fe1'), ('INFO', '2023-07-04 12:31:12,136 - distributed.worker - INFO - Starting Worker plugin PreImport-4d08c42e-8cd2-497c-8603-0ea550fe0245'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-347a43fa-1401-4c21-9314-8f3f2c51871b'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-g56t5vht'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:47569'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38873'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:39165'), ('INFO', '2023-07-04 12:31:11,778 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:39165'))}
+ sleep 1
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2551385)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 12:39:31,613 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2552572. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688467171.960554] [dgx-4:2552572:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:39:33,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:39:33,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-329' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39402.33150 ms
Code block 'ddf-preprocessing' took: 15018.01956 ms
Code block 'ddf-preprocessing' took: 15190.90878 ms
Code block 'ddf-preprocessing' took: 15198.08485 ms
Code block 'ddf-preprocessing' took: 15025.36005 ms
Code block 'ddf-scaler-fit' took: 18455.61593 ms
Code block 'ddf-scaler-fit' took: 15397.91709 ms
Code block 'ddf-scaler-fit' took: 15748.59788 ms
Code block 'ddf-scaler-fit' took: 16552.10223 ms
Code block 'ddf-scaler-fit' took: 17323.45438 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 48659.24292 ms
Code block 'ddf-kmeans-score' took: 15680.80725 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42598.51555 ms
Code block 'ddf-kmeans-score' took: 15794.28104 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42519.93393 ms
Code block 'ddf-kmeans-score' took: 15649.77355 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42576.64221 ms
Code block 'ddf-kmeans-score' took: 15993.22187 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42348.75089 ms
Code block 'ddf-kmeans-score' took: 15745.45284 ms
scores=[array(-4836113.34375)]
client.get_worker_logs()={'ucx://127.0.0.1:58141': (('INFO', "2023-07-04 12:47:27,748 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:47:00,611 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:46:28,307 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:46:01,100 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:45:28,855 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:45:01,654 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:44:29,432 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:44:02,213 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:43:29,902 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:42:58,903 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:39:34,965 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:39:34,965 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:50037'), ('INFO', '2023-07-04 12:39:34,884 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:39:34,884 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf9e0ca7-d19e-4355-bd03-e5d866ee3826'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO - Starting Worker plugin PreImport-34e80a68-a78b-465c-b26e-734ce8b94eb3'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b853d01-dc6d-4ad2-bde6-7012ba850f6d'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-fo0uo9_6'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:50037'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41337'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:58141'), ('INFO', '2023-07-04 12:39:33,934 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:58141'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2561008)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 12:48:11,954 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2562111. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688467692.360460] [dgx-4:2562111:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:48:13,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:48:13,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-329' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39380.69336 ms
Code block 'ddf-preprocessing' took: 15321.57364 ms
Code block 'ddf-preprocessing' took: 15439.03277 ms
Code block 'ddf-preprocessing' took: 15327.34738 ms
Code block 'ddf-preprocessing' took: 15381.28564 ms
Code block 'ddf-scaler-fit' took: 23263.04292 ms
Code block 'ddf-scaler-fit' took: 16823.51421 ms
Code block 'ddf-scaler-fit' took: 17125.39745 ms
Code block 'ddf-scaler-fit' took: 18030.16362 ms
Code block 'ddf-scaler-fit' took: 18648.69880 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 48626.72132 ms
Code block 'ddf-kmeans-score' took: 17992.05645 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44954.96289 ms
Code block 'ddf-kmeans-score' took: 17985.95546 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45078.79604 ms
Code block 'ddf-kmeans-score' took: 17993.62295 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45055.15965 ms
Code block 'ddf-kmeans-score' took: 17998.28840 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45136.00208 ms
Code block 'ddf-kmeans-score' took: 17994.44320 ms
scores=[array(-4839053.1171875)]
client.get_worker_logs()={'ucx://127.0.0.1:46707': (('INFO', "2023-07-04 12:56:38,406 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:56:11,114 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:55:34,183 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:55:06,919 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:54:30,048 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:54:02,754 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:53:25,870 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:52:58,600 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 12:52:21,597 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 12:51:50,672 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:48:15,443 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:48:15,443 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:45431'), ('INFO', '2023-07-04 12:48:15,357 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:48:14,422 - distributed.worker - INFO - Starting Worker plugin PreImport-191f37a8-f88f-402f-ba3d-87485846b39c'), ('INFO', '2023-07-04 12:48:14,422 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35e30aaf-7688-408f-849a-75c92da572c4'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO - Starting Worker plugin RMMSetup-deebfd37-c018-40fd-8711-f558eeacfd18'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-l_qyrbzo'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:45431'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33765'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:46707'), ('INFO', '2023-07-04 12:48:14,421 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:46707'))}
2023-07-04 12:57:01,236 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2571060)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 12:57:26,112 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2572141. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688468246.461195] [dgx-4:2572141:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 12:57:28,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 12:57:28,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-327' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39593.34995 ms
Code block 'ddf-preprocessing' took: 15100.68812 ms
Code block 'ddf-preprocessing' took: 15078.64692 ms
Code block 'ddf-preprocessing' took: 15177.41051 ms
Code block 'ddf-preprocessing' took: 15045.26966 ms
Code block 'ddf-scaler-fit' took: 16126.37240 ms
Code block 'ddf-scaler-fit' took: 11685.71904 ms
Code block 'ddf-scaler-fit' took: 12021.36580 ms
Code block 'ddf-scaler-fit' took: 12249.30771 ms
Code block 'ddf-scaler-fit' took: 12473.28389 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43270.16303 ms
Code block 'ddf-kmeans-score' took: 12490.02650 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39339.44559 ms
Code block 'ddf-kmeans-score' took: 12523.38351 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39369.74313 ms
Code block 'ddf-kmeans-score' took: 12427.44373 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39493.80450 ms
Code block 'ddf-kmeans-score' took: 12561.85851 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39327.44692 ms
Code block 'ddf-kmeans-score' took: 12336.55435 ms
scores=[array(-4838474.0625)]
client.get_worker_logs()={'ucx://127.0.0.1:32869': (('INFO', "2023-07-04 13:04:31,853 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:04:04,649 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:03:38,901 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:03:11,661 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:02:45,950 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:02:18,674 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:01:52,909 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:01:25,663 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:00:59,875 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:00:29,026 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 12:57:29,466 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:57:29,466 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:50985'), ('INFO', '2023-07-04 12:57:29,386 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:57:29,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b174958-3a38-4654-8b81-dcaff16bec88'), ('INFO', '2023-07-04 12:57:28,432 - distributed.worker - INFO - Starting Worker plugin PreImport-a1c2da75-94f1-4cce-807c-5640ea9b1fe5'), ('INFO', '2023-07-04 12:57:28,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-207bf849-853d-4b24-ae5c-4d6d7d455759'), ('INFO', '2023-07-04 12:57:28,432 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-n3_g6ffu'), ('INFO', '2023-07-04 12:57:28,432 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:50985'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38657'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:32869'), ('INFO', '2023-07-04 12:57:28,431 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:32869'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2579635)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 13:05:12,922 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2580724. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688468713.281909] [dgx-4:2580724:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 13:05:14,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:05:14,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-325' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 38692.36290 ms
Code block 'ddf-preprocessing' took: 15221.47004 ms
Code block 'ddf-preprocessing' took: 15113.55671 ms
Code block 'ddf-preprocessing' took: 15182.59134 ms
Code block 'ddf-preprocessing' took: 15100.91861 ms
Code block 'ddf-scaler-fit' took: 18551.92690 ms
Code block 'ddf-scaler-fit' took: 14362.18528 ms
Code block 'ddf-scaler-fit' took: 14634.96390 ms
Code block 'ddf-scaler-fit' took: 14839.91831 ms
Code block 'ddf-scaler-fit' took: 15042.61793 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 45655.70374 ms
Code block 'ddf-kmeans-score' took: 15067.29619 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42165.93950 ms
Code block 'ddf-kmeans-score' took: 15056.86027 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42084.43141 ms
Code block 'ddf-kmeans-score' took: 15081.33591 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42213.09461 ms
Code block 'ddf-kmeans-score' took: 15186.13895 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42126.29245 ms
Code block 'ddf-kmeans-score' took: 15285.46146 ms
scores=[array(-4837183.75)]
client.get_worker_logs()={'ucx://127.0.0.1:51099': (('INFO', "2023-07-04 13:12:54,488 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:12:27,215 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:11:56,137 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:11:28,890 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:10:57,810 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:10:30,619 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:09:59,642 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:09:32,514 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:09:01,211 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:08:30,471 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:05:16,243 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:05:16,243 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:58847'), ('INFO', '2023-07-04 13:05:16,164 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:05:16,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-823e398c-3eb9-4fa0-8796-3c7664e70876'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO - Starting Worker plugin PreImport-9c23d771-493a-4a0d-898b-5f4be8aa5c5e'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67acaecb-7875-4dcd-9d4a-c8445f5a3d0d'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-vghbnkzl'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:58847'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35509'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:51099'), ('INFO', '2023-07-04 13:05:15,244 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:51099'))}
+ sleep 1
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2588916)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 13:13:37,540 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2590012. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688469217.909725] [dgx-4:2590012:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 13:13:39,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:13:39,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-351' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 41243.89502 ms
Code block 'ddf-preprocessing' took: 15327.16045 ms
Code block 'ddf-preprocessing' took: 15396.91782 ms
Code block 'ddf-preprocessing' took: 15329.80940 ms
Code block 'ddf-preprocessing' took: 15403.63372 ms
Code block 'ddf-scaler-fit' took: 16651.97534 ms
Code block 'ddf-scaler-fit' took: 12703.96461 ms
Code block 'ddf-scaler-fit' took: 13178.10918 ms
Code block 'ddf-scaler-fit' took: 13351.41128 ms
Code block 'ddf-scaler-fit' took: 13635.92982 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 43388.39740 ms
Code block 'ddf-kmeans-score' took: 12368.31736 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39432.35979 ms
Code block 'ddf-kmeans-score' took: 12352.92989 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39262.38541 ms
Code block 'ddf-kmeans-score' took: 12441.99877 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39454.24312 ms
Code block 'ddf-kmeans-score' took: 12419.20469 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 39479.25565 ms
Code block 'ddf-kmeans-score' took: 12473.05574 ms
scores=[array(-4838335.8828125)]
client.get_worker_logs()={'ucx://127.0.0.1:57317': (('INFO', "2023-07-04 13:20:51,021 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:20:23,793 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:19:58,108 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:19:30,895 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:19:05,173 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:18:37,943 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:18:12,470 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:17:45,252 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:17:19,498 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:16:48,522 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:13:41,267 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:13:41,267 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:40117'), ('INFO', '2023-07-04 13:13:41,190 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:13:41,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f1ef01d-ae92-4584-8145-1202a1945616'), ('INFO', '2023-07-04 13:13:40,253 - distributed.worker - INFO - Starting Worker plugin PreImport-fced3755-ddb2-4c40-8de4-5442a2f8f1f0'), ('INFO', '2023-07-04 13:13:39,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c66abe7f-93d5-43e4-93de-ec12035d47e7'), ('INFO', '2023-07-04 13:13:39,878 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-bwqoft1s'), ('INFO', '2023-07-04 13:13:39,878 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:40117'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46097'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:57317'), ('INFO', '2023-07-04 13:13:39,877 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:57317'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2598543)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --rmm-pool-size 0.7 --mp-blocksize 256MiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 13:21:31,669 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2599615. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688469692.111255] [dgx-4:2599615:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 13:21:33,471 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:21:33,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-338' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 39436.19274 ms
Code block 'ddf-preprocessing' took: 15403.00497 ms
Code block 'ddf-preprocessing' took: 15459.65284 ms
Code block 'ddf-preprocessing' took: 15371.48428 ms
Code block 'ddf-preprocessing' took: 15450.80895 ms
Code block 'ddf-scaler-fit' took: 19270.57797 ms
Code block 'ddf-scaler-fit' took: 15200.83125 ms
Code block 'ddf-scaler-fit' took: 15478.97493 ms
Code block 'ddf-scaler-fit' took: 15989.66486 ms
Code block 'ddf-scaler-fit' took: 16126.41679 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 46071.82027 ms
Code block 'ddf-kmeans-score' took: 14944.77271 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42001.72931 ms
Code block 'ddf-kmeans-score' took: 15019.26855 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41959.91244 ms
Code block 'ddf-kmeans-score' took: 15326.14365 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41967.61829 ms
Code block 'ddf-kmeans-score' took: 15116.52815 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 42079.72148 ms
Code block 'ddf-kmeans-score' took: 15115.52575 ms
scores=[array(-4838401.1953125)]
client.get_worker_logs()={'ucx://127.0.0.1:41959': (('INFO', "2023-07-04 13:29:19,638 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:28:52,426 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:28:21,404 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:27:54,233 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:27:23,070 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:26:55,867 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:26:25,062 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:25:57,832 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:25:27,063 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:24:56,055 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:21:35,228 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:21:35,228 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:45527'), ('INFO', '2023-07-04 13:21:35,159 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:21:34,231 - distributed.worker - INFO - Starting Worker plugin PreImport-d214ef5e-5270-4654-b644-467f58e149d1'), ('INFO', '2023-07-04 13:21:34,231 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-774616c0-3c84-467d-baa9-72759cac8a58'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO - Starting Worker plugin RMMSetup-054a6385-eae2-419f-8aa8-42a93c88069c'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-_sdzvyoe'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:45527'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38793'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:41959'), ('INFO', '2023-07-04 13:21:33,879 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:41959'))}
2023-07-04 13:29:39,349 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
+ sleep 1
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2607547)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 13:30:01,457 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2608609. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688470201.847427] [dgx-4:2608609:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 13:30:03,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:30:03,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 38303.65486 ms
Code block 'ddf-preprocessing' took: 15249.43485 ms
Code block 'ddf-preprocessing' took: 15128.50251 ms
Code block 'ddf-preprocessing' took: 15261.02920 ms
Code block 'ddf-preprocessing' took: 15159.59364 ms
Code block 'ddf-scaler-fit' took: 15237.04330 ms
Code block 'ddf-scaler-fit' took: 10990.99902 ms
Code block 'ddf-scaler-fit' took: 11018.21642 ms
Code block 'ddf-scaler-fit' took: 11231.22343 ms
Task exception was never retrieved
future: <Task finished name='Task-325' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-scaler-fit' took: 11259.26566 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 41764.71069 ms
Code block 'ddf-kmeans-score' took: 10913.05009 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37853.57735 ms
Code block 'ddf-kmeans-score' took: 10825.67510 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37816.82170 ms
Code block 'ddf-kmeans-score' took: 10858.14425 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37964.39820 ms
Code block 'ddf-kmeans-score' took: 10990.33786 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 37832.97632 ms
Code block 'ddf-kmeans-score' took: 10882.40865 ms
scores=[array(-4836863.3125)]
client.get_worker_logs()={'ucx://127.0.0.1:38329': (('INFO', "2023-07-04 13:36:47,295 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:36:20,144 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:35:57,459 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:35:30,314 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:35:07,618 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:34:40,471 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:34:17,953 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:33:50,798 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:33:27,964 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:32:57,047 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:30:04,782 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:30:04,782 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:33001'), ('INFO', '2023-07-04 13:30:04,715 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:30:03,786 - distributed.worker - INFO - Starting Worker plugin PreImport-53dc9af7-e68d-445e-ad1b-ddbbc7ddb0e7'), ('INFO', '2023-07-04 13:30:03,786 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81b8abbc-d522-494e-8070-e553f7f998e1'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-746199cd-c3ce-40f2-b1ac-100d337fe986'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-hqb1t_qu'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:33001'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39161'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:38329'), ('INFO', '2023-07-04 13:30:03,500 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:38329'))}
+ sleep 1
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ ucx == ucx ]]
+ FILES=California.json
+ PROTOCOL=ucx
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE='--rmm-pool-size 0.7'
+ MP_BLOCKSIZE=1GiB
+ MP_PINNED_READ=--mp-pinned-read
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2615861)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files California.json --reps 5 --protocol ucx --rmm-pool-size 0.7 --mp-blocksize 1GiB --mp-force-host-read --mp-pinned-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=['California.json'], reps=5, protocol='ucx', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=0.7, jit_unspill=False, mp_blocksize='1GiB', mp_force_host_read=True, mp_pinned_read=True, mp_force_gpu_preprocess=False)
2023-07-04 13:37:24,605 - distributed.comm.ucx - WARNING - A CUDA context for device 0 (b'GPU-6ac8c492-e4bb-a92c-0a7c-4bacb5127bf4') already exists on process ID 2616920. This is often the result of a CUDA-enabled library calling a CUDA runtime function before Dask-CUDA can spawn worker processes. Please make sure any such function calls don't happen at import time or in the global scope of a program.
[1688470644.961980] [dgx-4:2616920:0]          parser.c:1908 UCX  WARN  unused env variable: UCX_MEMTYPE_CACHE (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2023-07-04 13:37:26,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:37:26,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Task exception was never retrieved
future: <Task finished name='Task-350' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Code block 'ddf-preprocessing' took: 37756.04365 ms
Code block 'ddf-preprocessing' took: 15092.80184 ms
Code block 'ddf-preprocessing' took: 15176.40617 ms
Code block 'ddf-preprocessing' took: 15103.49078 ms
Code block 'ddf-preprocessing' took: 15078.57880 ms
Code block 'ddf-scaler-fit' took: 18203.78018 ms
Code block 'ddf-scaler-fit' took: 13636.37628 ms
Code block 'ddf-scaler-fit' took: 13696.44561 ms
Code block 'ddf-scaler-fit' took: 13769.90596 ms
Code block 'ddf-scaler-fit' took: 14051.69858 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 44340.26313 ms
Code block 'ddf-kmeans-score' took: 13524.68416 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40527.69397 ms
Code block 'ddf-kmeans-score' took: 13473.50585 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40476.81191 ms
Code block 'ddf-kmeans-score' took: 13538.47097 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40465.40765 ms
Code block 'ddf-kmeans-score' took: 13655.12420 ms
Fitting kmeans with 8 clusters
Code block 'ddf-kmeans-fit' took: 40602.93539 ms
Code block 'ddf-kmeans-score' took: 13523.02716 ms
scores=[array(-4836340.34375)]
client.get_worker_logs()={'ucx://127.0.0.1:45345': (('INFO', "2023-07-04 13:44:47,194 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:44:20,071 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:43:51,931 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:43:24,807 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:42:56,928 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:42:29,815 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:42:01,940 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:41:34,811 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', "2023-07-04 13:41:06,713 - distributed.worker - INFO - Run out-of-band function '_func_destroy_all'"), ('INFO', "2023-07-04 13:40:35,918 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:37:28,270 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:37:28,270 - distributed.worker - INFO -         Registered to:      ucx://127.0.0.1:37271'), ('INFO', '2023-07-04 13:37:28,203 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:37:28,203 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48daa133-bd61-4a24-baf4-becca63440af'), ('INFO', '2023-07-04 13:37:27,259 - distributed.worker - INFO - Starting Worker plugin PreImport-d00e0dd3-2310-4a06-b58f-0612e3afd3ce'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e5f79f82-6499-4abc-964b-02d08786454d'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-88qu4nz_'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:37:26,928 - distributed.worker - INFO - Waiting to connect to:      ucx://127.0.0.1:37271'), ('INFO', '2023-07-04 13:37:26,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32843'), ('INFO', '2023-07-04 13:37:26,927 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:37:26,927 - distributed.worker - INFO -          Listening to:      ucx://127.0.0.1:45345'), ('INFO', '2023-07-04 13:37:26,927 - distributed.worker - INFO -       Start worker at:      ucx://127.0.0.1:45345'))}
+ sleep 1
+ for FILES in "California.json" ""
+ for PROTOCOL in tcp ucx
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ --enable-infiniband == '' ]]
+ for ENABLE_IB in "--enable-infiniband" ""
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ --enable-nvlink == '' ]]
+ for ENABLE_NVLINK in "--enable-nvlink" ""
+ for RMM_POOL_SIZE in "" "--rmm-pool-size 0.7"
+ for MP_BLOCKSIZE in "256MiB" "1GiB"
+ for MP_PINNED_READ in "" "--mp-pinned-read"
+ [[ tcp == ucx ]]
+ [[ '' == '' ]]
+ [[ '' == '' ]]
+ FILES=
+ PROTOCOL=tcp
+ ENABLE_IB=
+ ENABLE_NVLINK=
+ RMM_POOL_SIZE=
+ MP_BLOCKSIZE=256MiB
+ MP_PINNED_READ=
+ MP_FORCE_HOST_READ=--mp-force-host-read
+ singularity run --nv -B /scratch/shared/pwesolowski /home2/faculty/pwesolowski/containers/cuml-prod.sif /bin/bash --rcfile /home2/faculty/pwesolowski/containers/singularity_rc -ci /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/run_no_gds.sh
[WARN  tini (2624510)] Tini is not running as PID 1 .
Zombie processes will not be re-parented to Tini, so zombie reaping won't work.
To fix the problem, run Tini as PID 1.
This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.download.nvidia.com/licenses/NVIDIA_Deep_Learning_Container_License.pdf

bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
+ cd /home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/
+ python -u cuml_single_node.py --files --reps 5 --protocol tcp --mp-blocksize 256MiB --mp-force-host-read
<Managed Device 0>
args=Namespace(data_dir='/scratch/shared/pwesolowski/mgr-pipeline/joined-cuml', files=[], reps=5, protocol='tcp', enable_infiniband=False, enable_nvlink=False, rmm_pool_size=None, jit_unspill=False, mp_blocksize='256MiB', mp_force_host_read=True, mp_pinned_read=False, mp_force_gpu_preprocess=False)
2023-07-04 13:45:26,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-04 13:45:26,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
Code block 'ddf-preprocessing' took: 208610.41227 ms
Code block 'ddf-preprocessing' took: 186896.59050 ms
Code block 'ddf-preprocessing' took: 187068.26361 ms
Code block 'ddf-preprocessing' took: 186870.93361 ms
Code block 'ddf-preprocessing' took: 187558.59620 ms
Code block 'ddf-scaler-fit' took: 173232.43147 ms
Code block 'ddf-scaler-fit' took: 173636.08155 ms
Code block 'ddf-scaler-fit' took: 177713.03745 ms
Code block 'ddf-scaler-fit' took: 182922.61920 ms
Code block 'ddf-scaler-fit' took: 187941.69640 ms
Fitting kmeans with 8 clusters
2023-07-04 14:19:36,140 - distributed.worker - WARNING - Compute Failed
Key:       _func_fit-ac29136b-971d-45ab-bb61-312a8defcb86
Function:  _func_fit
args:      (b'\xfe\x1cf\xd1\xcd&C\x99\xbf\xb0\x95fa\xe4\x88:', [         rating  latitude  longitude
0          0.00  0.656013   0.301301
1          0.00  0.656013   0.301301
2          0.75  0.655884   0.301487
3          1.00  0.655884   0.301487
4          1.00  0.655884   0.301487
...         ...       ...        ...
1185819    1.00  0.649850   0.301483
1185820    1.00  0.649850   0.301483
1185821    0.50  0.649850   0.301483
1185822    0.75  0.649850   0.301483
1185823    0.75  0.649850   0.301483

[1185824 rows x 3 columns],          rating  latitude  longitude
0          0.75  0.649850   0.301483
1          1.00  0.649850   0.301483
2          0.00  0.649850   0.301483
3          1.00  0.649850   0.301483
4          0.75  0.649850   0.301483
...         ...       ...        ...
1156339    0.75  0.651414   0.301539
1156340    1.00  0.651414   0.301539
1156341    1.00  0.651414   0.301539
1156342    1.00  0.651414   0.301539
1156343    0.75  0.651414   0.301539

[1156344 rows x 3 columns],  
kwargs:    {'max_iter': 100, 'tol': 1e-10, 'n_clusters': 8, 'random_state': 1, 'verbose': False}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory')"

Code block 'ddf-kmeans-fit' took: 191330.30277 ms
client.get_worker_logs()={'tcp://127.0.0.1:38349': (('WARNING', '2023-07-04 14:19:36,140 - distributed.worker - WARNING - Compute Failed\nKey:       _func_fit-ac29136b-971d-45ab-bb61-312a8defcb86\nFunction:  _func_fit\nargs:      (b\'\\xfe\\x1cf\\xd1\\xcd&C\\x99\\xbf\\xb0\\x95fa\\xe4\\x88:\', [         rating  latitude  longitude\n0          0.00  0.656013   0.301301\n1          0.00  0.656013   0.301301\n2          0.75  0.655884   0.301487\n3          1.00  0.655884   0.301487\n4          1.00  0.655884   0.301487\n...         ...       ...        ...\n1185819    1.00  0.649850   0.301483\n1185820    1.00  0.649850   0.301483\n1185821    0.50  0.649850   0.301483\n1185822    0.75  0.649850   0.301483\n1185823    0.75  0.649850   0.301483\n\n[1185824 rows x 3 columns],          rating  latitude  longitude\n0          0.75  0.649850   0.301483\n1          1.00  0.649850   0.301483\n2          0.00  0.649850   0.301483\n3          1.00  0.649850   0.301483\n4          0.75  0.649850   0.301483\n...         ...       ...        ...\n1156339    0.75  0.651414   0.301539\n1156340    1.00  0.651414   0.301539\n1156341    1.00  0.651414   0.301539\n1156342    1.00  0.651414   0.301539\n1156343    0.75  0.651414   0.301539\n\n[1156344 rows x 3 columns],  \nkwargs:    {\'max_iter\': 100, \'tol\': 1e-10, \'n_clusters\': 8, \'random_state\': 1, \'verbose\': False}\nException: "MemoryError(\'std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory\')"\n'), ('INFO', "2023-07-04 14:19:26,884 - distributed.worker - INFO - Run out-of-band function '_func_init_all'"), ('INFO', '2023-07-04 13:45:28,626 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:45:28,626 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32829'), ('INFO', '2023-07-04 13:45:28,597 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO - Starting Worker plugin PreImport-0ae3c578-01ed-4f37-afb9-3f050cc5f08e'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cabd1c7b-07ae-4553-88ea-057eabaed9d5'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6762252-a70a-4cc5-a10d-f9712fbeda29'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -       Local Directory: /scratch/shared/pwesolowski/mgr-pipeline/joined-cuml/tmp/dask-worker-space/worker-w5_6jeq_'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -                Memory:                  40.00 GiB'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -               Threads:                          2'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO - -------------------------------------------------'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32829'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40767'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -           Worker name:                          0'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38349'), ('INFO', '2023-07-04 13:45:27,669 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38349'))}
2023-07-04 14:19:40,019 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
Traceback (most recent call last):
  File "/home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/cuml_single_node.py", line 106, in <module>
    raise e
  File "/home2/faculty/pwesolowski/praca-mgr/pipelines-repo/1_dataset_cuml/cuml_single_node.py", line 93, in <module>
    kmeans.fit(ddf)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/cuml/internals/memory_utils.py", line 85, in cupy_rmm_wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/cuml/dask/cluster/kmeans.py", line 170, in fit
    wait_and_raise_from_futures(kmeans_fit)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/cuml/dask/common/utils.py", line 159, in wait_and_raise_from_futures
    raise_exception_from_futures(futures)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/cuml/dask/common/utils.py", line 148, in raise_exception_from_futures
    raise RuntimeError("%d of %d worker jobs failed: %s" % (
RuntimeError: 1 of 1 worker jobs failed: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory
slurmstepd: error: *** JOB 688144 ON dgx-4 CANCELLED AT 2023-07-04T18:49:40 ***
